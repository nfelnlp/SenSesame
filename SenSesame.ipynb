{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SenSesame.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqsBFxrkOFsm",
        "colab_type": "text"
      },
      "source": [
        "### Sentiment Analysis on IMDb Movie Reviews dataset performed with a barebone reimplementation of the NER task of huggingface's BERT/transformers package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdycnZKsbG8I",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoUu931AZ9Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install seqeval transformers\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import spacy\n",
        "import tarfile\n",
        "import time\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import urllib.request as urlr\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers.data.processors.utils import InputExample\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from tqdm import tqdm, trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0SO43aubKif",
        "colab_type": "text"
      },
      "source": [
        "**Download the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHxlc81EbGXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Paths\n",
        "data_url = 'http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "data_tar_path = 'aclImdb_v1.tar.gz'\n",
        "\n",
        "if not os.path.exists(data_tar_path):\n",
        "    urlr.urlretrieve(data_url, data_tar_path)\n",
        "    tar = tarfile.open(data_tar_path)\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqPosI46bpaY",
        "colab_type": "text"
      },
      "source": [
        "**Initialize BERT model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXxLTZJlaKXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Args\n",
        "model_type = 'bert-base-cased'\n",
        "cache_dir = 'models'\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = ppb.BertTokenizer.from_pretrained(model_type, cache_dir=cache_dir)\n",
        "\n",
        "# Model\n",
        "model = ppb.BertForSequenceClassification.from_pretrained(model_type, cache_dir=cache_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGzi2E4kcgbP",
        "colab_type": "text"
      },
      "source": [
        "**Method to read in single files from dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GynMjUXfcjQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'aclImdb'\n",
        "label_subdirs = ['neg', 'pos']\n",
        "\n",
        "\n",
        "def read_data_from_directory(data_dir, mode, perc=1.0):\n",
        "    \"\"\" Modified version from transformers/examples/utils_ner.py for datasets with one example per file \"\"\"\n",
        "    docs = []\n",
        "    idx = 1\n",
        "    for sub in label_subdirs:\n",
        "        sub_files = [os.path.join(data_dir, sub, f) for f in os.listdir(os.path.join(data_dir, sub))]\n",
        "        cutoff = int(len(sub_files) * perc)\n",
        "        random.shuffle(sub_files)\n",
        "        if perc < 1.0:\n",
        "            sub_files = sub_files[:cutoff]\n",
        "        print('Processing {} files for {}'.format(len(sub_files), mode))\n",
        "        for doc in sub_files:\n",
        "            contents = open(doc).read()\n",
        "            # Remove linebreaks\n",
        "            contents = contents.replace('<br /><br />', '')\n",
        "            # Remove title\n",
        "            contents = contents.split('*******')[0]\n",
        "            \n",
        "            # Remove label if test\n",
        "            if mode == 'test':\n",
        "                label = None\n",
        "            \n",
        "            docs.append(InputExample(guid='{}-{}'.format(mode, idx),\n",
        "                                     text_a=contents,\n",
        "                                     label=sub))\n",
        "            idx += 1\n",
        "    return docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_ICr5jJ8W7N",
        "colab_type": "text"
      },
      "source": [
        "**Feature converter** (from utils_ner.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfsDUYan8cSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label = label\n",
        "\n",
        "def convert_examples_to_features(examples,\n",
        "                                 label_list,\n",
        "                                 max_seq_length,\n",
        "                                 tokenizer,\n",
        "                                 cls_token_at_end=False,\n",
        "                                 cls_token=\"[CLS]\",\n",
        "                                 cls_token_segment_id=1,\n",
        "                                 sep_token=\"[SEP]\",\n",
        "                                 sep_token_extra=False,\n",
        "                                 pad_on_left=False,\n",
        "                                 pad_token=0,\n",
        "                                 pad_token_segment_id=0,\n",
        "                                 pad_token_label_id=-1,\n",
        "                                 sequence_a_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 100 == 0:\n",
        "            print(\"Writing example {} of {}\".format(ex_index, len(examples)))\n",
        "\n",
        "        tokens = []\n",
        "        label = label_map[example.label]\n",
        "        # Apply spacy tokenization to every document\n",
        "        doc = spacy_nlp(example.text_a)\n",
        "        for word in [token.text for token in doc]:\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            tokens.extend(word_tokens)\n",
        "\n",
        "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "        special_tokens_count = 3 if sep_token_extra else 2\n",
        "        if len(tokens) > max_seq_length - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids:   0   0   0   0  0     0   0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens += [sep_token]\n",
        "        if sep_token_extra:\n",
        "            # roberta uses an extra separator b/w pairs of sentences\n",
        "            tokens += [sep_token]\n",
        "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        if cls_token_at_end:\n",
        "            tokens += [cls_token]\n",
        "            segment_ids += [cls_token_segment_id]\n",
        "        else:\n",
        "            tokens = [cls_token] + tokens\n",
        "            segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        if pad_on_left:\n",
        "            input_ids = ([pad_token] * padding_length) + input_ids\n",
        "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "        else:\n",
        "            input_ids += ([pad_token] * padding_length)\n",
        "            input_mask += ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "            segment_ids += ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if ex_index < 3:\n",
        "            print(\"*** Example ***\")\n",
        "            print(\"guid: %s\", example.guid)\n",
        "            print(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
        "            print(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
        "            print(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
        "            print(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
        "            print(\"label: {}\".format(label))\n",
        "\n",
        "        \n",
        "            \n",
        "        features.append(\n",
        "                InputFeatures(input_ids,\n",
        "                              input_mask,\n",
        "                              segment_ids,\n",
        "                              label))\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7vhIZ5SuB-t",
        "colab_type": "text"
      },
      "source": [
        "**Prepare training and test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo9y7livt_bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 128\n",
        "labels = ['neg', 'pos']\n",
        "all_data = []  # List of DataLoaders\n",
        "\n",
        "for subset in ['train', 'test']:\n",
        "    data_partition = read_data_from_directory(os.path.join(data_dir, subset), subset, perc=.02)\n",
        "    features = convert_examples_to_features(data_partition, labels, max_seq_length, tokenizer)\n",
        "    \n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_labels)\n",
        "    \n",
        "    if subset == 'train':\n",
        "        train_batch_size = 32\n",
        "        train_sampler = RandomSampler(dataset)\n",
        "        dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
        "    else:\n",
        "        eval_batch_size = 8\n",
        "        eval_sampler = SequentialSampler(dataset)\n",
        "        dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
        "        \n",
        "    all_data.append(dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRFedzIQpJhl",
        "colab_type": "text"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0tZOckmpLfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function\n",
        "def evaluate(dataloader):\n",
        "    # Variables\n",
        "    ev_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    true_labels, preds = [], []\n",
        "    \n",
        "    model.eval()\n",
        "    for batch in tqdm(dataloader, desc='Evaluating'):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Forward propagation\n",
        "            inputs = {\"input_ids\": batch[0],\n",
        "                      \"attention_mask\": batch[1],\n",
        "                      \"token_type_ids\": batch[2],\n",
        "                      \"labels\": batch[3]}\n",
        "            out = model(**inputs)\n",
        "            # out : tuple of loss and logits\n",
        "\n",
        "            tmp_eval_loss, logits = out[:2]\n",
        "            ev_loss += tmp_eval_loss.item()\n",
        "        nb_eval_steps += 1\n",
        "        \n",
        "        preds.extend(logits.detach().cpu().numpy())\n",
        "        true_labels.extend(inputs[\"labels\"].detach().cpu().numpy())\n",
        "        \n",
        "    ev_loss /= nb_eval_steps\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    \n",
        "    eval_metrics = classification_report(true_labels, preds, output_dict=True)['weighted avg']\n",
        "        \n",
        "    results = {\n",
        "        \"loss\": ev_loss,\n",
        "        \"precision\": eval_metrics['precision'],\n",
        "        \"recall\": eval_metrics['recall'],\n",
        "        \"f1\": eval_metrics['f1-score']}\n",
        "    \n",
        "    for key in sorted(results.keys()):\n",
        "        print(\"{} : {}\".format(key, str(results[key])))\n",
        "        \n",
        "    return results, preds\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iSD8gBQYg2o",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wTxuwPUYpbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training params\n",
        "gradient_acc_steps = 1\n",
        "num_epochs = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data\n",
        "train_dataloader = all_data[0]\n",
        "\n",
        "# Optimizer\n",
        "optimizer = ppb.AdamW(model.parameters())\n",
        "scheduler = ppb.WarmupLinearSchedule(optimizer, warmup_steps=0, t_total=1 + (len(dataloader) // gradient_acc_steps))\n",
        "\n",
        "global_step = 0\n",
        "tr_loss, logging_loss = 0.0, 0.0\n",
        "model.zero_grad()\n",
        "\n",
        "tr_it = trange(int(num_epochs), desc='Epoch', ascii=True)\n",
        "\n",
        "for _ in tr_it:\n",
        "    epoch_it = tqdm(train_dataloader, desc='Iteration', ascii=True)\n",
        "    \n",
        "    for step, batch in enumerate(epoch_it):\n",
        "        time.sleep(0.01)\n",
        "        model.train()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Forward propagation\n",
        "        inputs = {\"input_ids\": batch[0],\n",
        "                  \"attention_mask\": batch[1],\n",
        "                  \"token_type_ids\": batch[2],\n",
        "                  \"labels\": batch[3]}\n",
        "        out = model(**inputs)\n",
        "        # out : tuple of loss and logits\n",
        "        \n",
        "        # Backpropagate loss\n",
        "        loss = out[0]\n",
        "        loss.backward()\n",
        "        tr_loss += loss.item()\n",
        "        \n",
        "        if (step + 1) % gradient_acc_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
        "            \n",
        "            # Update scheduler, optimizer\n",
        "            scheduler.step()\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "            \n",
        "            # Evaluate\n",
        "            evaluate(all_data[1])\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}